{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97cb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8f1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_file:インプットファイル名。拡張子付きで記載し、このノートブックと同じディレクトリに作成した./data/ディレクトリ配下にファイルを置く。\n",
    "　　　　　　　　　　　　　　　　　テキストが記載されている列のヘッダーをtextとすること\n",
    "encode:csvのエンコード方法を記載\n",
    "\"\"\"\n",
    "\n",
    "input_file = 'shonen_tanteidan.csv'\n",
    "encode = 'sjis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa83760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv('./data/' + input_file, encoding=encode)\n",
    "result_df = input_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e476f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wakati(result_df):\n",
    "    targets = result_df['text']\n",
    "    target_arr = np.array([])\n",
    "    target_arr2 = np.array([])\n",
    "    wakati = MeCab.Tagger('-Owakati')\n",
    "    meishi = MeCab.Tagger('wakati')\n",
    "\n",
    "    for target in targets:\n",
    "        #単純なわかちがき\n",
    "        target_arr = np.append(target_arr, wakati.parse(target))\n",
    "\n",
    "        #名詞のみ抽出するわかちがき\n",
    "        result = meishi.parse(target)\n",
    "        lines = result.split('\\n')\n",
    "        work_arr = np.array([])\n",
    "\n",
    "        for line in lines:\n",
    "            feature = line.split('\\t')\n",
    "            if len(feature) == 2: #'EOS'と''を省く\n",
    "                info = feature[1].split(',')\n",
    "                hinshi = info[0]\n",
    "                if hinshi in ('名詞'):\n",
    "                    work_arr = np.append(work_arr, info[6])\n",
    "        noun_txt = ' '.join(work_arr)\n",
    "        target_arr2 = np.append(target_arr2,noun_txt)\n",
    "\n",
    "    result_df['w_text'] = target_arr\n",
    "    result_df['noun_only'] = target_arr2\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6bef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = wakati(result_df)\n",
    "\n",
    "output_df.to_csv('./data/' + input_file[:-4] + '_wakati.csv', encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
